[model]
type = "seqdistmodel"
package = "models.mamba"

[scaling]
strategy = "pa"

[standardisation]
standardise = 1
mean = 93.69239463939118
stdev = 23.506745239082388

[run_info]
sample_type = "dna"
sample_rate = 5000

[qscore]
scale = 1.05
bias = 1.3

[basecaller]
batchsize = 128
chunksize = 5000
overlap = 100

[training]
command = "train"
training_directory = "./squigulator/models/new_dataset/batch_size_64/mamba_512_v4_lr_2.5e-4/"
config = "./models/mamba_512_new//mamba_512_v4.toml"
pretrained = ""
directory = "PosixPath('squigulator/dataset/train/npy/train')"
device = "cuda"
lr = "2.5e-4"
seed = 25
epochs = 2
batch = 64
chunks = 0
no_amp = false
force = true
restore_optim = false
nondeterministic = false
save_optim_every = 10
grad_accum_split = 1
quantile_grad_clip = false
num_workers = 4
optuna = false
func = "<function main at 0x7fba42cc3490>"

[model.seqdist]
state_len = 5
alphabet = [ "N", "A", "C", "G", "T",]

[model.encoder]
type = "namedserial"

[model.encoder.conv]
type = "serial"
[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 1
size = 64
bias = true
winlen = 5
stride = 1
padding = 2
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 64
size = 64
bias = true
winlen = 5
stride = 1
padding = 2
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 64
size = 128
bias = true
winlen = 9
stride = 3
padding = 4
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 128
size = 128
bias = true
winlen = 9
stride = 2
padding = 4
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "convolution"
insize = 128
size = 512
bias = true
winlen = 5
stride = 2
padding = 2
activation = "swish"
norm = "batchnorm"

[[model.encoder.conv.sublayers]]
type = "permute"
dims = [ 0, 2, 1,]

[model.encoder.transformer_encoder]
type = "stack"
depth = 3

[model.encoder.upsample]
type = "linearupsample"
d_model = 512
scale_factor = 2

[[model.encoder.conv.sublayers]]
type = "permute"
dims = [ 1, 0, 2,]

[model.encoder.transformer_encoder.layer]
type = "mambalayer2"
d_model = 512
nhead = 8
d_state = 64
headdim = 64
d_conv = 4
nlayer = 1
chunk_size = 512
dim_feedforward = 2048
deepnorm_alpha = 2.4494897
deepnorm_beta = 0.2886751
